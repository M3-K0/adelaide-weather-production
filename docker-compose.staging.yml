# Adelaide Weather Forecasting System - Staging Environment
version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: api/Dockerfile.production
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=staging
      - API_TOKEN=${API_TOKEN:-staging-token}
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:3000}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - RATE_LIMIT_PER_MINUTE=${RATE_LIMIT_PER_MINUTE:-120}
      - PERFORMANCE_CACHE_TTL=${PERFORMANCE_CACHE_TTL:-180}
    volumes:
      # Mount core system components
      - ./core:/app/core:ro
      - ./outcomes:/app/outcomes:ro
      - ./indices:/app/indices:ro
      - ./embeddings:/app/embeddings:ro
      - ./models:/app/models:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 1.5G
        reservations:
          cpus: '0.5'
          memory: 512M
    networks:
      - staging-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.production
    environment:
      - NODE_ENV=staging
      - API_BASE_URL=http://api:8000
      - API_TOKEN=${API_TOKEN:-staging-token}
      - NEXT_TELEMETRY_DISABLED=1
    depends_on:
      api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - staging-network

  nginx:
    image: nginx:1.25-alpine
    ports:
      - "80:80"
      - "443:443"  # For future SSL support
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - staging_nginx_logs:/var/log/nginx
    depends_on:
      api:
        condition: service_healthy
      frontend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    networks:
      - staging-network

  # Redis for caching (staging environment)
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --maxmemory 128mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_staging_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 3s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 32M
    networks:
      - staging-network

  # Monitoring (optional with --profile monitoring)
  prometheus:
    image: prom/prometheus:v2.45.0
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_staging_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=7d'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 64M
    networks:
      - staging-network
    profiles:
      - monitoring

  grafana:
    image: grafana/grafana:10.0.0
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-staging}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel
    volumes:
      - grafana_staging_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 64M
    networks:
      - staging-network
    profiles:
      - monitoring

networks:
  staging-network:
    driver: bridge
    name: adelaide-staging

volumes:
  prometheus_staging_data:
    driver: local
    name: adelaide-staging-prometheus
  grafana_staging_data:
    driver: local
    name: adelaide-staging-grafana
  redis_staging_data:
    driver: local
    name: adelaide-staging-redis
  staging_nginx_logs:
    driver: local
    name: adelaide-staging-nginx-logs

# Staging-specific configuration:
# - Production-like services but with reduced resource allocation
# - Separate network namespace for isolation
# - Nginx reverse proxy with SSL-ready configuration
# - Monitoring available via profiles
# - Redis for caching validation
# - Health checks with staging-appropriate timeouts
#
# Access points:
# http://localhost        - Main application via Nginx proxy
# http://localhost/api/*  - API routes proxied to FastAPI backend
# https://localhost       - HTTPS support (when SSL certificates are configured)